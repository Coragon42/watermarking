For the frequency distribution analysis experiment, we used 500 prompts from the OpenGen dataset, which are paragraph completion tasks. For each of the Soft and Unigram watermarks (meaning we did this experiment twice), we inputted those 500 prompts into both the watermarked and unwatermarked models, added up occurrences per token to get watermarked and unwatermarked token frequency distributions (so these are not to be confused with the probability distributions used during generation), and we then subtracted the token frequencies between the watermarked and unwatermarked distributions. 

Then we tested the assumption that the 50 largest positive frequency differences were for green tokens and that the 50 largest negative frequency differences were for red tokens (so these graphs are truncated). The intuition behind this assumption is that the green bias should lead to green tokens being selected more often, leading to a higher frequency of green tokens in the watermarked output (so a positive frequency difference), and likewise the red tokens should have a lower frequency in the watermarked output than the unwatermarked output (which is a negative difference). But this assumption is not trivial because for all LLMs, the probability distributions for each next token are always conditional on all the previous tokens, so it's not like the tokens generated are just simply repeatedly selected from the same probability distribution.

What we found was that the assumption was 70% accurate for Soft (meaning 80% of the top 50 differences were green and 60% of the bottom 50 were red) and it was 98% accurate for Unigram (100% of the top 50 were green and 96% of the bottom 50 were red). It also looked like Unigram's fixed green and red lists resulted in larger frequency differences overall, as you can see in the x-axis scales. This may have been because the Soft watermark's lists changing for every token allowed the same token to be green on one instance and red on another within the same output.

Also note how Soft had more specific words in the largest differences (like big, anime, Mediterranean), while Unigram had more universally common words (like in, The, was), which again makes sense because common words inherently occur frequently for linguistic reasons, and for Unigram tokens always have constant greenness or redness.

We did also try alternative prompt choices, like the LQFA dataset (which are basically Reddit ELI5 posts), and we also tried repeating the same prompt 500 times, which we thought might be more effective on Unigram because maybe it would reduce the variety of tokens in the outputs, but OpenGen ended up working the best for the highest accuracy of this experiment.